{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A model of cooperation between agents, based on the the explicit partner choice model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "As stated in [1], a game theoretic setting to study cooperation even in an evolutionary context can be interesting and useful. Usually in literature, following again [1], it is used the Prisoner's Dilemma (PD) to describe a dyadic and only once simulated interaction between agents, while for an evolutionary setting it is preferred the Iterated Prisoner's Dilemma (IPD). Yet, interestingly, there are problems arising from this choice. A part from some theoretical challenges, such as the need to introduce errors in the choices of the agents or such as the dependence of the spread of strategies on the geometry of the problems and on the distribution of the population, there are even few critical empirical problems, as Silk suggests in [5].\n",
    "\n",
    "The most interesting observation is that the IPD-winning Tit-For-Tat (TFT) strategy, which consists of deciding to act on each turn as the confronting agent did in the previous one, having cooperated in the first turn, does not  fully described some kind of cooperation. In particular, it does not fully grasp long-term relations between human beings. Infact, the pure mechanism of keeping track of all the moves of the other cooperating agent does not characterize those human relations that are not easily provoked into defection by a partner's occasional defection, a part from eventually an initial phase of mistrust, when a TFT strategy can in some way describe the situation.\n",
    "Moreover partners who reach a cooperation will tend to stick together more than the one who do not reach it and so strategies will not meet with a probability equal to their current representation in the population, as  IPD's models assumes typically.\n",
    "Therefore, the authors propose a different model for the simulation of cooperation between two individuals, that of the explicit partner choice model, with some additional features to overcome previous errors. \n",
    "\n",
    "This same choice is made in this work following as Hruschka and Henrich the comments in [5] about the IPD's short-sightedness for simulating more complex human cooperative interactions, such as friendships, and having decided to build a model in which cooperation between human beings is one of the strategies to survive and to complete easy tasks.  \n",
    "\n",
    "At the same time many works, including [6] and [4], suggest that Graph Theory is a natural and very convenient framework to describe the population structure on which the evolution of cooperation is studied. Therefore in the following model while simulation is running, the program builds a network based on the interactions between agents, so that later it can be analyzed to see if the conclusions made in the cited paper are consistent with the model built."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The working of the model\n",
    "The description of the basic model is as similar as possible to the one of [3]. A certain number of agents, given as input by the observer, is created and for a number of cycles, decided again by the observer, interacts in a dyadic PD game. \n",
    "At the first cycle the agents choose their partner randomly, but after the first time they try to choose their partner with whom interact between those belonging to their memory. In fact each agent has a matrix memory of finite capacity, decided at the beginning by the observer. The agent stocks in its memory the _name_ of the agents that encounters with a particular procedure, explained afterwards.\n",
    "\n",
    "They decide how to interact with their partner on the base of the parameter _alwaysDefect_ from here on $ D $. If $ D $ is one, then, a part from a error in the strategy, the agent chooses to defect, while if D is zero the agent, again a part from an error in the strategy choice, decides what to do on the basis of its memory. If the agent has its memory full he'll cope with the partner, only if a random value is above a certain threshold. The latter is represented by another parameter of the model named _cliqueshness_, from here on $ \\chi $. On the contrary if it still has space in its memory he'll cope with the agent, if it does not make a mistake in the strategy choice.\n",
    "\n",
    "Once having decided how to play in the PD, they do play according to the following benefit-cost matrix, whose benefit-cost rate $(\\dfrac{B}{C})$ is decided by the observer at the before the simulation starts. \n",
    "\n",
    "|                  |$\\alpha$ cooperates           |$\\alpha$ defects |\n",
    "|       ---        |       ---                    |      ---        |\n",
    "|$\\beta$ cooperates|$\\dfrac{B}{C} * 100$          |$0$                |\n",
    "|$\\beta$ defects   |$\\dfrac{B}{C} * 100 + 100$    |$100$              |\n",
    "\n",
    "where $\\alpha$ and $\\beta$ are two agents from the agent set.\n",
    "\n",
    "The third action that each agent does is to update its status. First of all, the fitness $ F_{jk} $ of the agent j in the turn k is $ F_{jk} = p_{jk} + \\alpha \\ F_{jk-1} $, where $ p_{jk} $ is the agent's payoff and $ \\alpha $ is a common parameter generally fixed at 0.6. \n",
    "Secondly, if the turn's partner is already in the memory of the agent then the preference $ pref_{ijk} $ between the two agents (i and j) at the turn k. Is updated as $ pref_{ijk} = \\delta  pref_{ijk-1} + (1-\\delta)  M_{ijk}$, where $ \\delta $ stands for the parameter \\textit{pastWeighting} and $ M_{ijk} $ is agent's payoff of the turn divided by the maximum payoff possible.\n",
    "Instead, if the interaction between the two is new and if they have collaborated, the agent remembers of its partner, if its memory is not full or if the preference is better than at least one of those memorized.\n",
    "When the memory is full, each agent will forget about those other agents with a collaboration preference below the fourth parameter: $ \\tau $ or the _thresholdCoop_.\n",
    "\n",
    "Finally, the last stage of the simulation begins. A function chooses two agents: one to die and the other to reproduce. \n",
    "Each agent has the probability of dying of $ P_{jk}(\\textrm{death}) = 1 - \\dfrac{F_{ik}-\\min(F_{})}{\\max(F_{})-\\min(F_{})} $ and of reproducing of $ P_{jk}(\\textrm{reproduce}) = \\dfrac{F_{ik}}{\\sum F_{ik}}$. In the case that the sum of fitnesses is zero or their extrema are equal then all agents have the same probability of dying or of reproducing.\n",
    "When an agent is chosen to die all other agents delete it from their memories and it will not be anymore in the list of agent, while if it reproduces itself, a new agent is created with the four main parameters as its parent's ones with the addition of a little variability given by the mutation of the three continuous parameters by a normal distribution with mean and variance (0, 0.05) and of $ D $ with probability 0.05. \n",
    "\n",
    "There's no real need of giving a spatial distribution to the agents, so no proper world is created during the simulation. Yet, as wrote above, while simulating the model builds and shows the network of the relation of agents. It is a directed and weighted graph, with each node representing a living agent from which a directed link departs to reach each agent stored in its memory. For a better visualization the graph is similar to the one of [6], but with links in three different width and colors (yellow, orange, red) to mark the increasing weight of the edges.\n",
    "\n",
    "When the simulation is finished there is the possibility of calculating some statistics about the model, with the possibility to visualize it thanks to some plots about the distribution of the agents' parameters.\n",
    "After that, the program provides some basic network analysis on the agents' relations' graph.\n",
    "\n",
    "Another thing to notice is that at the beginning of the model the observer is also asked if it desires to choose the distribution of the parameters in the population, so that certain strategies can arise and either spread or cease to exist. The strategy to choose are parametrized as the following: \n",
    "\n",
    "|            | pure defector | pure cooperator | pure cooperator (no memory) | out for tat | cliquer | \n",
    "| ---        | ---           |     ---         | ---                         | ---         |    ---  |\n",
    "|$D$         | 1             |    0            |   0                         |   0         |     0   |\n",
    "|$\\delta$    |    any        |    any          |        any                  |   0         |     0.5 |\n",
    "|$\\tau$      |       any     |     any         |     $\\tau > 1-\\delta$       |    0.5      |    0.15 |\n",
    "|$\\chi$      |       any     |      0          |       0                     |       0     |     1   |\n",
    "\n",
    "\n",
    "To sum up a part from those values that the observer inserts at the beginning of the simulation four other parameters, at the moment randomly distributed govern the model:\n",
    "\n",
    "  -$ D\\; (D=0,1)$: it is the parameter that decides whether the agent has the inclination to defect or cope.\n",
    "    \n",
    "  -$\\delta\\; (0\\le\\delta\\le1)$: it is the parameter that gives a weight to the experience of the agents, therefore its inclination to ignore or to value the past.\n",
    "    \n",
    "  -$ \\tau \\;(0\\le\\tau\\le1)$: it is the parameter that measures the preferences of the interactions with other partners. If the value of interaction with a determined partner is below the threshold, it is forgot.\n",
    "\n",
    "  -$ \\chi\\; (0\\le\\chi\\le1)$: it is the probability that an agent will defect when his memory is full. That is to say its inclination towards the formation of its circle of \\textit{friends}."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results and further developments\n",
    "\n",
    "A necessary premise must be made before the analysis of the results of this work. For a complete analysis of the results of this present model and of the consistency with that of [3], the program should provide the implementation of a statistical analysis between the trials of simulation and a direct comparison with the original model should be presented. \n",
    "\n",
    "The first thing to notice is that in the model there are many variables that play a determining role in the final results: the distribution of the strategies, the two types of errors, the entries in the payoff matrix. These factors could influence in a sensible way the result of the model, as described in [3].\n",
    "The program provides both a statistical analysis and a network analysis to help to understand the results of the simulation. \n",
    "\n",
    "In the statistical analysis' section six plots are presented. The first five regard the distribution of the parameters in the world, and the last one tries to count directly the number of strategies in the model. To have a total perspective of the evolutionary behaviour of the strategies one must take into account all six of them. In fact, if the last one gives a proof that the parameters do evolve and shows in which direction, at the same time, since some strategies, as that of cliquers, are based on parameters strictly defined as above, and since we introduced mutation in the evolution, the direct accounting of the strategies during the simulation cannot be exact. The plot counts only those strategies that remain parametrized as at the beginning. Yet, in the model parameters do change because of the continuous mutations. Therefore, inspecting the changes of the distribution in the previous five plots is the complete way to understand whether a strategy is winning or not. \n",
    "\n",
    "Instead from the network analysis, we find that usually the out degrees of the nodes are higher that the in degrees, or more generally the former are more uniformly distributed, while the latter have a sharper distribution around the number corresponding to the memory capacity of the agents. From the histogram that tells the weights distribution on the edges, as one could expect, we get a correlation of the distribution with the quantity of defectors in the model. In fact, if the defectors' number is not significant, we usually get that the most of edges have its weghts higher that 0.5, with peaks around 0.8 or 0.9, while if we increase the number of defectors in the model, the strenght of the ties between the agents decrease, with many weights less than 0.5. \n",
    "In the last cell of the programm there are some general features of the graph. From the simulations, we get that generally there are two connected components, and since one of them is represented only by the newly born node of the last agent created, it means that the graph is basically totally connected. That is to say that not providing restrains to the possibility of the agents to know the world around them causes a world totally connected. \n",
    "To complete this information, the analysis provides the distribution of the clustering and betwenness centrality parameters, that both tend to be low during the simulation done. The network of agents does not have marked groups of separeted nodes, and no node seems to be more important than others, if the sample of population is big enough(at least 100 individuals).\n",
    "Moreover, the biggest subset of nodes of the undirected copy of the original graph that are all adjacent, also called maximum clique, is made usually of three or four nodes, even in simulation of 1000 agents, with mainly cooperators or cliquers.  This result could be a way of expressing that in the current model there is not the possibility of creating a large or medium size group of friends.\n",
    "\n",
    "The simulations were run with several types of samples. Their extension was of 40, 100, 400 and occasionally 1000 agents. The other parameters were varied the most possible, in order to try to see which was the winning strategy in which setting, and whether a single strategy could grow out of a small inintial population. The result were similar to those in [3]. About the pure defector strategy, it is found out that it is stronger when the benefic/cost ratio is small, as one could imagine. The Out-For-Tat strategy is not particularly strong, while in many cases, even if it was not observed a direct ousting of pure defector strategies, the cliquers strategy seems a strong one. In particular, the cliqueshness parameter $\\chi$ increased in the distribution of the model indipendently from the size of the sample.\n",
    "\n",
    "To conclude, if the strategical behaviour seems to be consistent with that of the original paper, some questions can arise from the reading of [3] and from the model here presented. \n",
    "First of all, does the model proosed fully describe the simulation of the formation of cliques of \"friends\"? Or should further features  be introduced to enhance the group formation? This question is to be answered basing the deductions on anthropoligical studies.\n",
    "\n",
    "Yet, more interestigly, from a pure economic and social point of view a second question arises: are these groups good units to merge into for agents that want to survive to a particular setting? If cliquers' strategy is a winning and stable strategy in a neutral setting, when confronted with more difficult task, does it keep the same strenght? For example, can groups of friends collaborate with other groups of friends or is the mistrust for strangers to strong? \n",
    "For reasons of time and opportunity this further aspect was not ivestigated, leaving it for further studies. But having in mind [2], a work proposing a model to challenge the latter question could have been presented. I believe that a simple implementation of the payoff matrix, the creation of some new realistic strategies and the creation of a particular setting could be enough to develop an answer to this question. The setting could be a world where there are three different technologies, all of which will be compatible in some way with the survival of the agents, but at the same time with certain drawbacks, calculated as effects on parameters of each agent.\n",
    "\n",
    "Therefore, this work helps to investigate into the problem of computing some qualitative and general features of close human relationships and of reciprocity. Its main use could therefore be to complete an analysis on the creation and evolution of more complex and higher level social organization and phenomena. Evidently, since this work has not much added to [3] from a qualitative point of view, its main contribute is the use of Jupyter for the programming. This has updated the model, in order to make it again usable, and it made it more compatible with further formal analysis or with implementation by the observer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    " [1] Axelrod, Robert and Hamilton, William D. The evolution of cooperation. *science*,211(4489):1390-1396, 1981.\n",
    "\n",
    " [2] Farjam, Mike Daniel and Faillo, Marco and Sprinkhuizen-Kuyper, Ida G and Haselager, WFG. Punishment mechanisms and their effect on cooperation: A simulation study. 2015\n",
    " \n",
    " [3] Hruschka, Daniel J and Henrich, Joseph. Friendship, cliquishness, and the emergence of cooperation. *Journal of theoretical biology*, 239(1):1-15, 2006.\n",
    " \n",
    " [4] Santos, Francisco C and Pacheco, Jorge M.Scale-free networks provide a unifying framework for the emergence of cooperation. *Physical Review Letters*,95(9):098104, 2005.\n",
    "\n",
    " [5] Silk, Joan B. Cooperation without counting. *Genetic and cultural evolution of cooperation*, pages 37-54, 2003.\n",
    " \n",
    " [6] Zachary, Wayne W. An information flow model for conflict and fission in small groups. *Journal of anthropological research*, 33(4):452-473, 1977.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To use the model, execute all the cells.\n",
    "During the simulation you will be asked to insert some parameters on which the simulation will be based, as described in the section \"The working of the model\". \n",
    "First you will have choose the general parameters of the setting. Here is a table with some suggested values, from which you could start in your first simulations. \n",
    "\n",
    "| Parameter | Suggested values |\n",
    "| ---  |    ---           |\n",
    "|Number of agents | 40, 100, 400, 1000 |\n",
    "|Memory capacity  | 4, 6, 8, 12        |\n",
    "|Error in the strategy choice | 0, 0.05, 0.1|\n",
    "|Error in the pairing | 0, 0.05, 0.1|\n",
    "|Benefit-cost ratio | 2 5 7 9 |\n",
    "|Cycles of simulation | at least 200 |\n",
    "\n",
    "\n",
    "Then, you will be asked whether you want to choose the distribution of the strategies in the world created. If so,  look at the sections above for further informations. \n",
    "\n",
    "A last, there are three ways of running the model depending on the speed of the simulation. The only difference between them is how and when the graph of the agents' relations will be plotted. In the slow version, it will be plotted at each iteration, it is advisable to use it only for small populations of agents. In the medium version the plot will be displayed every 50 iterations and in the fast version only at the end of the simulation.\n",
    "\n",
    "\n",
    "The first cell is there to load functions from libraries and other notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import networkx as net\n",
    "from IPython import display\n",
    "import random as rd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from math import sin, cos, pi\n",
    "import io, os, sys, types\n",
    "from IPython import get_ipython\n",
    "from nbformat import read\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "def find_notebook(fullname, path=None):\n",
    "    \"\"\"find a notebook, given its fully qualified name and an optional path\n",
    "\n",
    "    This turns \"foo.bar\" into \"foo/bar.ipynb\"\n",
    "    and tries turning \"Foo_Bar\" into \"Foo Bar\" if Foo_Bar\n",
    "    does not exist.\n",
    "    \"\"\"\n",
    "    name = fullname.rsplit('.', 1)[-1]\n",
    "    if not path:\n",
    "        path = ['']\n",
    "    for d in path:\n",
    "        nb_path = os.path.join(d, name + \".ipynb\")\n",
    "        if os.path.isfile(nb_path):\n",
    "            return nb_path\n",
    "        # let import Notebook_Name find \"Notebook Name.ipynb\"\n",
    "        nb_path = nb_path.replace(\"_\", \" \")\n",
    "        if os.path.isfile(nb_path):\n",
    "            return nb_path\n",
    "        \n",
    "class NotebookLoader(object):\n",
    "    \"\"\"Module Loader for Jupyter Notebooks\"\"\"\n",
    "    def __init__(self, path=None):\n",
    "        self.shell = InteractiveShell.instance()\n",
    "        self.path = path\n",
    "\n",
    "    def load_module(self, fullname):\n",
    "        \"\"\"import a notebook as a module\"\"\"\n",
    "        path = find_notebook(fullname, self.path)\n",
    "\n",
    "        print (\"importing Jupyter notebook from %s\" % path)\n",
    "\n",
    "        # load the notebook object\n",
    "        with io.open(path, 'r', encoding='utf-8') as f:\n",
    "            nb = read(f, 4)\n",
    "\n",
    "\n",
    "        # create the module and add it to sys.modules\n",
    "        # if name in sys.modules:\n",
    "        #    return sys.modules[name]\n",
    "        mod = types.ModuleType(fullname)\n",
    "        mod.__file__ = path\n",
    "        mod.__loader__ = self\n",
    "        mod.__dict__['get_ipython'] = get_ipython\n",
    "        sys.modules[fullname] = mod\n",
    "\n",
    "        # extra work to ensure that magics that would affect the user_ns\n",
    "        # actually affect the notebook module's ns\n",
    "        save_user_ns = self.shell.user_ns\n",
    "        self.shell.user_ns = mod.__dict__\n",
    "\n",
    "        try:\n",
    "            for cell in nb.cells:\n",
    "                if cell.cell_type == 'code':\n",
    "                    # transform the input to executable Python\n",
    "                    code = self.shell.input_transformer_manager.transform_cell(cell.source)\n",
    "                    # run the code in themodule\n",
    "                    exec(code, mod.__dict__)\n",
    "        finally:\n",
    "            self.shell.user_ns = save_user_ns\n",
    "        return mod\n",
    "class NotebookFinder(object):\n",
    "    \"\"\"Module finder that locates Jupyter Notebooks\"\"\"\n",
    "    def __init__(self):\n",
    "        self.loaders = {}\n",
    "\n",
    "    def find_module(self, fullname, path=None):\n",
    "        nb_path = find_notebook(fullname, path)\n",
    "        if not nb_path:\n",
    "            return\n",
    "\n",
    "        key = path\n",
    "        if path:\n",
    "            # lists aren't hashable\n",
    "            key = os.path.sep.join(path)\n",
    "\n",
    "        if key not in self.loaders:\n",
    "            self.loaders[key] = NotebookLoader(path)\n",
    "        return self.loaders[key]\n",
    "sys.meta_path.append(NotebookFinder())\n",
    "\n",
    "\n",
    "import modelFunctions as modf\n",
    "import classPerson as clP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#interaction with the observer\n",
    "howManyAgents = eval(input(\"How many agents?  [Insert an integer] \"))\n",
    "memCap = 1 + eval(input(\"What is the memory capacity of the agents? [Insert an integer] \"))\n",
    "strErr = eval(input(\"With which probability can an agent choose the wrong action? [Insert a floating number between 0 and 1] \"))\n",
    "matErr = eval(input(\"With which probability can an agent choose the wrong partner?  [Insert a floating number between 0 and 1] \"))\n",
    "benCostR = eval(input(\"What is the benefit-cost ratio in the Prisoner's Dilemma matrix? [Insert an integer] \"))\n",
    "lenghtOfSim = eval(input(\"How many cycles of simulation? [Insert an integer] \"))\n",
    "uniformDistro = str(input(\"Do you  want to choose the strategy distribution between the agents? [y/n] \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counterOfInput = 0\n",
    "if uniformDistro == 'y':\n",
    "    pureDefectors = int(input('How many agents will prefer a pure defector strategy? [Insert an integer] '))\n",
    "    counterOfInput = pureDefectors\n",
    "    print(\"Until now, you have chosen \", counterOfInput, \" strategies out of a population of \", howManyAgents, \" agents.\")\n",
    "    \n",
    "    pureCoop = int(input('How many agents will prefer a pure cooperator strategy?  [Insert an integer] '))\n",
    "    counterOfInput += pureCoop\n",
    "    print(\"Until now, you have chosen \", counterOfInput, \" strategies out of a population of \", howManyAgents, \" agents.\")\n",
    "    \n",
    "    pureCoopNM = int(input('How many agents will prefer a pure cooperator (no memory) strategy?  [Insert an integer] '))\n",
    "    counterOfInput += pureCoopNM\n",
    "    print(\"Until now, you have chosen \", counterOfInput, \" strategies out of a population of \", howManyAgents, \" agents.\")\n",
    "    \n",
    "    OFT = int(input('How many agents will prefer a Out-for-Tat strategy?  [Insert an integer] '))\n",
    "    counterOfInput += OFT\n",
    "    print(\"Until now, you have chosen \", counterOfInput, \" strategies out of a population of \", howManyAgents, \" agents.\")\n",
    "    \n",
    "    cliq = int(input('How many agents will prefer a Cliquer strategy?  [Insert an integer] '))\n",
    "    counterOfInput += cliq\n",
    "    print(\"Until now, you have chosen \", counterOfInput, \" strategies out of a population of \", howManyAgents, \" agents.\")\n",
    "    \n",
    "    totRequired = pureDefectors + pureCoop + pureCoopNM + OFT + cliq \n",
    "    if totRequired > howManyAgents: \n",
    "        print('Wait, there is something wrong. The total of strategies required exceeds the number of agents. Please run again the cell!')\n",
    "\n",
    "    elif totRequired < howManyAgents:\n",
    "        print('The total of strategies is inferior to the number of agents. Those for whom you did not choose a strategy will have random values of the parameters.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creation of the world\n",
    "worldRegistry = []\n",
    "agentGraph = net.DiGraph()\n",
    "\n",
    "\n",
    "if uniformDistro == 'y': \n",
    "    totAg = 0\n",
    "    for i in range(pureDefectors):\n",
    "        aPerson = clP.Person(totAg, np.matrix([[], []]), memCap, 0, 0, 1, rd.uniform(0, 1), rd.uniform(0, 1),\\\n",
    "                         rd.uniform(0, 1), matErr, strErr, True, True, True, True, rd.randint(0, howManyAgents-1), 0, 0)\n",
    "        worldRegistry.append(aPerson)\n",
    "        agentGraph.add_node(aPerson.number, label = aPerson.number)\n",
    "        totAg += 1\n",
    "    for j in range(pureCoop):\n",
    "        aPerson = clP.Person(totAg, np.matrix([[], []]), memCap, 0, 0, 0, rd.uniform(0, 1), rd.uniform(0, 1),\\\n",
    "                         0, matErr, strErr, True, True, True, True, rd.randint(0, howManyAgents-1), 0, 0)\n",
    "        worldRegistry.append(aPerson)\n",
    "        agentGraph.add_node(aPerson.number, label = aPerson.number)        \n",
    "        totAg += 1\n",
    "    for y in range(pureCoopNM):\n",
    "        ru = rd.uniform(0,1)\n",
    "        aPerson = clP.Person(totAg, np.matrix([[], []]), memCap, 0, 0, 0, ru, rd.uniform(1-ru, 1),\\\n",
    "                       0, matErr, strErr, True, True, True, True, rd.randint(0, howManyAgents-1), 0, 0)\n",
    "        worldRegistry.append(aPerson)\n",
    "        agentGraph.add_node(aPerson.number, label = aPerson.number)\n",
    "        totAg += 1\n",
    "    for n in range(OFT):\n",
    "        aPerson = clP.Person(totAg, np.matrix([[], []]), memCap, 0, 0, 0, 0, 0.5,\\\n",
    "                       0, matErr, strErr, True, True, True, True, rd.randint(0, howManyAgents-1), 0, 0)\n",
    "        worldRegistry.append(aPerson)\n",
    "        agentGraph.add_node(aPerson.number, label = aPerson.number)\n",
    "        totAg += 1\n",
    "    for n in range (cliq):\n",
    "        aPerson = clP.Person(totAg, np.matrix([[], []]), memCap, 0, 0, 0, 0.5, 0.15,\\\n",
    "                       1, matErr, strErr, True, True, True, True, rd.randint(0, howManyAgents-1), 0, 0)\n",
    "        worldRegistry.append(aPerson)\n",
    "        agentGraph.add_node(aPerson.number, label = aPerson.number)\n",
    "        totAg += 1\n",
    "        \n",
    "    while totAg < howManyAgents: \n",
    "        aPerson = clP.Person(totAg, np.matrix([[], []]), memCap, 0, 0, rd.randint(0, 1), rd.uniform(0, 1), rd.uniform(0, 1),\\\n",
    "                        rd.uniform(0, 1), matErr, strErr, True, True, True, True, rd.randint(0, howManyAgents-1), 0, 0)\n",
    "        worldRegistry.append(aPerson)\n",
    "        agentGraph.add_node(aPerson.number, label = aPerson.number)\n",
    "        totAg += 1\n",
    "\n",
    "elif uniformDistro == 'n': \n",
    "    for i in range(howManyAgents): \n",
    "        aPerson = clP.Person(i, np.matrix([[], []]), memCap, 0, 0,rd.randint(0, 1), rd.uniform(0, 1), rd.uniform(0, 1),\\\n",
    "                         rd.uniform(0, 1), matErr, strErr, True, True, True, True, rd.randint(0, howManyAgents-1), 0, 0)\n",
    "        worldRegistry.append(aPerson)\n",
    "        agentGraph.add_node(aPerson.number, label = aPerson.number)\n",
    "\n",
    "#Pd's Payoff Matrix\n",
    "generalPayoffMatrix = np.matrix([[benCostR * 100, 0], [benCostR * 100 + 100, 100]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# a first recognition on the distribution of the parameters\n",
    "DsIn = []\n",
    "deltasIn = []\n",
    "tausIn = []\n",
    "chisIn = []\n",
    "paramDatas = []\n",
    "pDs = []\n",
    "pCs = []\n",
    "pCNMs = []\n",
    "oFTs = []\n",
    "cliQs = []\n",
    "for agent in worldRegistry: \n",
    "    DsIn.append(agent.alwaysDefect)\n",
    "    deltasIn.append(agent.pastWeighting)\n",
    "    tausIn.append(agent.thresholdCoop)\n",
    "    chisIn.append(agent.cliquishness)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "typeOfSim = str(input(\"Which type of simulation would you like to run? Please, insert one of the following strings: \\\n",
    "[slow | medium | fast] \"))\n",
    "if typeOfSim != 'slow' and typeOfSim != 'medium' and typeOfSim != 'fast': \n",
    "    print(\"There must have been an error while typing. Please run the cell again and choose one of the three values in the brackets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#the simulation\n",
    "tick = 0\n",
    "for tick in range(lenghtOfSim):\n",
    "    \n",
    "    listOfFs = []\n",
    "    listOfPs = []\n",
    "    lOfPairs = []\n",
    "    Ds = []\n",
    "    deltas = []\n",
    "    taus = []\n",
    "    chis = []\n",
    "    \n",
    "    edgeWidth = []\n",
    "    edgeCol = []\n",
    "    \n",
    "    if typeOfSim == 'slow' or typeOfSim == 'medium':\n",
    "        for (u,v,d) in agentGraph.edges(data=True):\n",
    "            if d['weight'] < 0.3:\n",
    "                edgeWidth.append(0.5)\n",
    "                edgeCol.append('yellow')\n",
    "            elif d['weight'] > 0.3 and d['weight'] < 0.7:\n",
    "                edgeWidth.append(1.5)\n",
    "                edgeCol.append('orange')\n",
    "            elif d['weight'] > 0.7:\n",
    "                edgeWidth.append(3)\n",
    "                edgeCol.append('red')\n",
    "    #the graph drawing\n",
    "    if typeOfSim == 'slow':\n",
    "        try:\n",
    "            plt.clf()\n",
    "            net.draw_circular(agentGraph, node_size =(300 / (howManyAgents/50)), width=edgeWidth,\\\n",
    "                              edge_color=edgeCol, node_color='blue', with_labels=True)\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        except KeyboardInterrupt:\n",
    "            break\n",
    "    if typeOfSim == 'medium':\n",
    "        if (tick % 50) == 0:\n",
    "            try:\n",
    "                plt.clf()\n",
    "                net.draw_circular(agentGraph, node_size =(300 / (howManyAgents/50)), width=edgeWidth,\\\n",
    "                                  edge_color=edgeCol, node_color='blue', with_labels=True)\n",
    "                display.display(plt.gcf())\n",
    "                display.clear_output(wait=True)\n",
    "            except KeyboardInterrupt:\n",
    "                break\n",
    "    #start of the actual cycle\n",
    "    rd.shuffle(worldRegistry)\n",
    "    for agent in worldRegistry:\n",
    "        if agent.amFree == True:\n",
    "            tickPartner = agent.choosePartner(worldRegistry) \n",
    "            lOfPairs.append([agent, tickPartner])\n",
    "        \n",
    "        agent.interact()\n",
    "        \n",
    "    for i in range(len(lOfPairs)):\n",
    "        modf.calculatePayoff(lOfPairs[i][0], lOfPairs[i][1], generalPayoffMatrix)    \n",
    "        \n",
    "    for agent in worldRegistry:\n",
    "        agent.update(generalPayoffMatrix)\n",
    "        for l in range(agent.partnersMemory.shape[1]):\n",
    "            agentGraph.add_edge(agent.number, agent.partnersMemory[0,l], weight = agent.partnersMemory[1,l])\n",
    "            \n",
    "        #updates the datas for the plots\n",
    "        Ds.append(agent.alwaysDefect)\n",
    "        deltas.append(agent.pastWeighting)\n",
    "        taus.append(agent.thresholdCoop)\n",
    "        chis.append(agent.cliquishness)\n",
    "    paramDatas.append([np.mean(Ds), np.mean(deltas), np.mean(taus), np.mean(chis)])\n",
    "    pDs.append(len([agent for agent in worldRegistry if agent.alwaysDefect == 1]))\n",
    "    pCs.append(len([agent for agent in worldRegistry if (agent.alwaysDefect == 0 and agent.cliquishness == 0)]))\n",
    "    pCNMs.append(len([agent for agent in worldRegistry if (agent.alwaysDefect == 0 and agent.cliquishness == 0\\\n",
    "                                                    and agent.thresholdCoop > 1 - agent.pastWeighting)]))\n",
    "    cliQs.append(len([agent for agent in worldRegistry if (agent.alwaysDefect == 0\\\n",
    "                                                  and agent.cliquishness == 1 and agent.pastWeighting == 0.5\\\n",
    "                                                  and agent.thresholdCoop == 0.15)]))\n",
    "    oFTs.append(len([agent for agent in worldRegistry if (agent.alwaysDefect == 0 and agent.pastWeighting == 0 and\\\n",
    "                                                   agent.cliquishness == 0 and agent.thresholdCoop == 0.5)]))\n",
    "    \n",
    "\n",
    "    #death and repr\n",
    "    listOfFs, listsOfPs = modf.listsOfProbs(worldRegistry, listOfFs, listOfPs)\n",
    "    l = modf.dieOrReprChoice(worldRegistry, listsOfPs)\n",
    "    worldRegistry = modf.die(l[1], worldRegistry)\n",
    "    agentGraph.remove_node(l[1].number)\n",
    "    D, delta, tau, chi = modf.varMutation(l[0].alwaysDefect, l[0].pastWeighting, l[0].thresholdCoop, l[0].cliquishness)\n",
    "    aNewPerson = clP.Person(len(worldRegistry) + tick + 1, np.matrix([[], []]), l[0].memoryCapacity, 0, 0, D, \\\n",
    "                        delta, tau, chi, l[0].pairingError, l[0].strategyError,\\\n",
    "                        l[0].doCooperate, l[0].didOtherCoop, True, True, l[0].number, 0,\\\n",
    "                        0)\n",
    "    worldRegistry.append(aNewPerson)\n",
    "    agentGraph.add_node(aNewPerson.number, label = aNewPerson.number)\n",
    "    \n",
    "print(\"A simulation is finished!\")\n",
    "#the graph drawing\n",
    "if typeOfSim == 'fast':\n",
    "    edgeWidth = []\n",
    "    edgeCol = []\n",
    "    for (u,v,d) in agentGraph.edges(data=True):\n",
    "        if d['weight'] < 0.3:\n",
    "            edgeWidth.append(0.5)\n",
    "            edgeCol.append('yellow')\n",
    "        elif d['weight'] > 0.3 and d['weight'] < 0.7:\n",
    "            edgeWidth.append(1.5)\n",
    "            edgeCol.append('orange')\n",
    "        elif d['weight'] > 0.7:\n",
    "            edgeWidth.append(3)\n",
    "            edgeCol.append('red')\n",
    "            \n",
    "\n",
    "    net.draw_circular(agentGraph, node_size =(300 / (howManyAgents/50)), width=edgeWidth,\\\n",
    "                              edge_color=edgeCol, node_color='blue', with_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The preparation of the results' analysis\n",
    "\n",
    "DsFin = Ds\n",
    "deltasFin = deltas\n",
    "tausFin = taus\n",
    "chisFin = chis\n",
    "zerosIn = 0\n",
    "zerosFin = 0\n",
    "onesIn = 0\n",
    "onesFin = 0\n",
    "for i in range(len(DsFin)):\n",
    "    \n",
    "    if DsFin[i] == 0:\n",
    "        zerosFin += 1\n",
    "    else:\n",
    "        onesFin += 1\n",
    "        \n",
    "    if DsIn[i] == 0:\n",
    "        zerosIn += 1\n",
    "    else:\n",
    "        onesIn += 1\n",
    "\n",
    "        \n",
    "meanD = []\n",
    "meanDeltas = []\n",
    "meanTaus = []\n",
    "meanChis = []\n",
    "for j in range(lenghtOfSim): \n",
    "    meanD.append(paramDatas[j][0])\n",
    "    meanDeltas.append(paramDatas[j][1])\n",
    "    meanTaus.append(paramDatas[j][2])\n",
    "    meanChis.append(paramDatas[j][3])\n",
    "\n",
    "\n",
    "deltaVal0 = []\n",
    "deltaVal1 = []\n",
    "tauVal0 = []\n",
    "tauVal1 = []\n",
    "chiVal0 = []\n",
    "chiVal1 = []\n",
    "for agent in worldRegistry:\n",
    "    if agent.alwaysDefect == 0: \n",
    "        deltaVal0.append(agent.pastWeighting)\n",
    "        tauVal0.append(agent.thresholdCoop)\n",
    "        chiVal0.append(agent.cliquishness)\n",
    "    else: \n",
    "        deltaVal1.append(agent.pastWeighting)\n",
    "        tauVal1.append(agent.thresholdCoop)\n",
    "        chiVal1.append(agent.cliquishness)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A breaf statistical analysis of the four parameters governing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following two couples are respectively the two distribution before the beginning of the simulation and at its end of the parameter alwaysDefect. The first value corresponds to the number of zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((zerosIn,onesIn))\n",
    "print((zerosFin, onesFin))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of the parameter pastWeighting at the beginning and at the end of the simulation had mean, variance, quintiles and extrema as below. The first set of values is referred to the beginning of the simulation the second to its end. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((np.mean(deltasIn), np.var(deltasIn)), (np.percentile(deltasIn, [20, 40, 60, 80])), (min(deltasIn), max(deltasIn)))\n",
    "      \n",
    "print((np.mean(deltasFin), np.var(deltasFin)), (np.percentile(deltasFin, [20, 40, 60, 80])), (min(deltasFin), max(deltasFin)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of the parameter thresholdCoop at the beginning and at the end of the simulation had mean, variance, quintiles and extrema as below. The first set of values is referred to the beginning of the simulation the second to its end. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print((np.mean(tausIn), np.var(tausIn)), (np.percentile(tausIn, [20, 40, 60, 80])), (min(tausIn), max(tausIn)))\n",
    "      \n",
    "print((np.mean(tausFin), np.var(tausFin)), (np.percentile(tausFin, [20, 40, 60, 80])), (min(tausFin), max(tausFin)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of the parameter cliquishness at the beginning and at the end of the simulation had mean, variance, quintiles and extrema as below. The first set of values is referred to the beginning of the simulation the second to its end. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((np.mean(chisIn), np.var(chisIn)), (np.percentile(chisIn, [20, 40, 60, 80])), (min(chisIn), max(chisIn)))\n",
    "      \n",
    "print((np.mean(chisFin), np.var(chisFin)), (np.percentile(chisFin, [20, 40, 60, 80])), (min(chisFin), max(chisFin)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = plt.figure()\n",
    "fig.suptitle(\"Simulation's plots\")\n",
    "fig, ax_lst = plt.subplots(2, 2, figsize = (20, 20))\n",
    "ax = ax_lst[0, 0]\n",
    "ax1 = ax_lst[0, 1]\n",
    "ax2 = ax_lst[1, 0]\n",
    "ax3 = ax_lst[1, 1]\n",
    "ax.set_title(\"The parameters' means' distribution in the simulation\")\n",
    "ax.plot(range(lenghtOfSim), meanD, color = 'lightblue', label = 'alwaysDefect')\n",
    "ax.plot(range(lenghtOfSim), meanDeltas, color = 'red', label = 'pastWeighting')\n",
    "ax.plot(range(lenghtOfSim), meanTaus, color = 'orange', label = 'thresholdCoop')\n",
    "ax.plot(range(lenghtOfSim), meanChis, color = 'green', label = 'cliquishness')\n",
    "ax.set_xlabel(\"time\")\n",
    "ax.set_ybound(lower=0, upper=1)\n",
    "ax.legend()\n",
    "\n",
    "ax1.set_title(\"The distribution of thresholdCoop by pastWeighting\")\n",
    "ax1.plot(deltaVal0, tauVal0, 'ro', label = 'cooperators')\n",
    "ax1.plot(deltaVal1, tauVal1, 'bo',  label = 'defectors')\n",
    "ax1.set_xlabel(\"pastWeighting\")\n",
    "ax1.set_ylabel(\"thresholdCoop\")\n",
    "ax1.set_ybound(lower=0, upper=1)\n",
    "ax1.legend()\n",
    "\n",
    "ax2.set_title(\"The distribution of cliquishness by pastWeighting\")\n",
    "ax2.plot(deltaVal0, chiVal0, 'ro', label = 'cooperators')\n",
    "ax2.plot(deltaVal1, chiVal1, 'bo',  label = 'defectors')\n",
    "ax2.set_xlabel(\"pastWeighting\")\n",
    "ax2.set_ylabel(\"cliquishness\")\n",
    "ax2.set_ybound(lower=0, upper=1)\n",
    "ax2.legend()\n",
    "\n",
    "ax3.set_title(\"The distribution of cliquishness by thresholdCoop\")\n",
    "ax3.plot(tauVal0, chiVal0, 'ro',  label = 'cooperators')\n",
    "ax3.plot(tauVal1, chiVal1, 'bo',  label = 'defectors')\n",
    "ax3.set_xlabel(\"thresholdCoop\")\n",
    "ax3.set_ylabel(\"cliquishness\")\n",
    "ax3.set_ybound(lower=0, upper=1)\n",
    "ax3.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2 = plt.figure()\n",
    "ax4 = fig2.add_subplot(111, projection='3d')\n",
    "ax4.scatter(deltaVal0, tauVal0, chiVal0, color = 'red', label = 'cooperator')\n",
    "ax4.scatter(deltaVal1, tauVal1, chiVal1, color = 'blue', label = 'defector')\n",
    "ax4.set_title(\"The distribution of all four parameters\")\n",
    "ax4.set_xlabel(\"pastWeighting\")\n",
    "ax4.set_ylabel(\"thresholdCoop\")\n",
    "ax4.set_zlabel(\"cliqueshness\")\n",
    "ax4.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig3 = plt.figure()\n",
    "ax4 = fig3.add_subplot(111)\n",
    "ax4.set_title(\"The distribution of strategies during the simulation\")\n",
    "ax4.set_xlabel('time')\n",
    "ax4.plot(range(lenghtOfSim), pDs, color = 'blue', label = 'Pure Defectors')\n",
    "ax4.plot(range(lenghtOfSim), pCs, color = 'red', label = 'Pure Cooperators')\n",
    "ax4.plot(range(lenghtOfSim), pCNMs, color = 'yellow', label = 'Pure Cooperators Without Memory')\n",
    "ax4.plot(range(lenghtOfSim), oFTs, color = 'green', label = 'Out For Tat')\n",
    "ax4.plot(range(lenghtOfSim), cliQs, color = 'black',label = 'Cliquers')\n",
    "ax4.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some basic network analysis of the graph created by the interaction of the agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, the distribution of the degrees and of the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inDeg = agentGraph.in_degree() \n",
    "inVal = sorted([v for (u,v) in inDeg])\n",
    "inHist = [inVal.count(x) for x in inVal]\n",
    "outDeg = agentGraph.out_degree()\n",
    "outVal = sorted([v for (u,v) in outDeg])\n",
    "outHist = [outVal.count(x) for x in outVal]\n",
    "wwList = [d['weight']for (u,v,d) in agentGraph.edges(data=True)]\n",
    "fig4 = plt.figure()\n",
    "fig4, ax_lst2 = plt.subplots(1, 2, figsize = (15, 7))\n",
    "ax5 = ax_lst2[0]\n",
    "ax5.set_title('The degree distribution in the graph')\n",
    "ax5.plot(inVal,inHist,'ro-')\n",
    "ax5.plot(outVal,outHist,'bo-') \n",
    "ax5.set_xlabel('Degree') \n",
    "ax5.set_ylabel('Number of Nodes')\n",
    "ax5.legend(['In-degree','Out-degree'])\n",
    "ax6 = ax_lst2[1]\n",
    "ax6.set_title(\"The distribution of weights in the graph's edges\")\n",
    "ax6.hist(wwList, 20)\n",
    "ax6.set_xlabel('Weights') \n",
    "ax6.set_ylabel('Number of Edges')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondly, some other values of the network, which are respectively:\n",
    "\n",
    "  1) mean, variance, quintiles and extrema of the clustering coefficient\n",
    "\n",
    "  2) the number of connected components \n",
    "\n",
    "  3) mean, variance, quintiles and extrema of the betweenness centrality\n",
    "\n",
    "  4) the biggest clique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "agentGraphUN = agentGraph.to_undirected()\n",
    "clCoeffs = [v for v in net.clustering(agentGraphUN).values()]\n",
    "aGphComponents = list(net.connected_component_subgraphs(agentGraphUN))\n",
    "btwCen = list(net.betweenness_centrality(agentGraph, weight = 'weight', normalized=True).values())\n",
    "cliqueList = list(net.node_clique_number(agentGraphUN).values())\n",
    "\n",
    "print('1) ',(np.mean(clCoeffs), np.var(clCoeffs)), (np.percentile(clCoeffs, [20, 40, 60, 80])), (min(clCoeffs), max(clCoeffs)))\n",
    "print('2) ',len(aGphComponents))\n",
    "print('3) ',(np.mean(btwCen), np.var(btwCen)), (np.percentile(btwCen, [20, 40, 60, 80])), (min(btwCen), max(btwCen)))\n",
    "print('4) ', max(cliqueList))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
